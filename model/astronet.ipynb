{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6ea0c3d",
   "metadata": {},
   "source": [
    "## Advanced feature engineering, ensembles, and robust evaluation\n",
    "- Leverage SHAP insights: add radius interactions and physically motivated ratios.\n",
    "- Explore FFT features (stub to plug in per-target FFT summary if available).\n",
    "- Optional PCA to reduce noise and collinearity.\n",
    "- Add stacking ensemble with RandomOverSampler to address class imbalance.\n",
    "- Add randomized hyperparameter search for GradientBoosting.\n",
    "- Integrate MC-robust evaluation end-to-end.\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80b390d",
   "metadata": {},
   "source": [
    "# AstroNet-inspired classifier on curated exoplanet dataset\n",
    "This notebook trains an MLP with dropout (AstroNet-style regularization) to classify exoplanet candidates using curated tabular features derived from light-curve meta-features and stellar/planetary parameters.\n",
    "\n",
    "Notes from literature:\n",
    "- Shallue & Vanderburg (2018) popularized AstroNet; dropout and data augmentation help generalization.\n",
    "- TESS/Kepler follow-ups advocate avoiding leakage (e.g., labels/flags not available at inference).\n",
    "- Impact parameter and duration/period features are physically informative; uncertainty should be leveraged via Monte Carlo aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42cb4559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: b:\\Akshat\\GIT and projects\\NASA_SapceApps\\Model\n",
      "Data path: b:\\Akshat\\GIT and projects\\NASA_SapceApps\\data\\unified_exoplanets_final_imputed.csv\n",
      "Artifacts dir: b:\\Akshat\\GIT and projects\\NASA_SapceApps\\Model\\artifacts\n"
     ]
    }
   ],
   "source": [
    "# Imports and setup\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    confusion_matrix,\n",
    "    precision_recall_curve,\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import clone\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Paths\n",
    "project_root = Path.cwd()\n",
    "data_path = project_root.parent / \"data\" / \"unified_exoplanets_final_imputed.csv\"\n",
    "artifacts_dir = project_root / \"artifacts\"\n",
    "\n",
    "print(\"Project root:\", project_root)\n",
    "print(\"Data path:\", data_path)\n",
    "print(\"Artifacts dir:\", artifacts_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05afd6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18716, 41)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akshat\\AppData\\Local\\Temp\\ipykernel_22908\\3443877944.py:3: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_path)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>planet_name</th>\n",
       "      <th>host_star_id</th>\n",
       "      <th>mission</th>\n",
       "      <th>disposition</th>\n",
       "      <th>orbital_period_days</th>\n",
       "      <th>transit_epoch_bjd</th>\n",
       "      <th>transit_duration_hours</th>\n",
       "      <th>transit_depth_ppm</th>\n",
       "      <th>planet_radius_re</th>\n",
       "      <th>equilibrium_temp_k</th>\n",
       "      <th>...</th>\n",
       "      <th>planet_radius_re_imputed</th>\n",
       "      <th>duration_over_period</th>\n",
       "      <th>duration_over_period_clipped</th>\n",
       "      <th>impact_parameter_pred_std</th>\n",
       "      <th>impact_parameter_ci95_low</th>\n",
       "      <th>impact_parameter_ci95_high</th>\n",
       "      <th>impact_parameter_imputed</th>\n",
       "      <th>equilibrium_temp_k_postfill_median</th>\n",
       "      <th>stellar_logg_postfill_median</th>\n",
       "      <th>stellar_teff_k_postfill_median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kepler-227 b</td>\n",
       "      <td>10797460</td>\n",
       "      <td>Kepler</td>\n",
       "      <td>CONFIRMED</td>\n",
       "      <td>9.488036</td>\n",
       "      <td>170.538750</td>\n",
       "      <td>2.9575</td>\n",
       "      <td>616.0</td>\n",
       "      <td>2.26</td>\n",
       "      <td>793.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.012988</td>\n",
       "      <td>0.012988</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kepler-227 c</td>\n",
       "      <td>10797460</td>\n",
       "      <td>Kepler</td>\n",
       "      <td>CONFIRMED</td>\n",
       "      <td>54.418383</td>\n",
       "      <td>162.513840</td>\n",
       "      <td>4.5070</td>\n",
       "      <td>875.0</td>\n",
       "      <td>2.83</td>\n",
       "      <td>443.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.003451</td>\n",
       "      <td>0.003451</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>K00753.01</td>\n",
       "      <td>10811496</td>\n",
       "      <td>Kepler</td>\n",
       "      <td>CANDIDATE</td>\n",
       "      <td>19.899140</td>\n",
       "      <td>175.850252</td>\n",
       "      <td>1.7822</td>\n",
       "      <td>10800.0</td>\n",
       "      <td>14.60</td>\n",
       "      <td>638.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.003732</td>\n",
       "      <td>0.003732</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    planet_name host_star_id mission disposition  orbital_period_days  \\\n",
       "0  Kepler-227 b     10797460  Kepler   CONFIRMED             9.488036   \n",
       "1  Kepler-227 c     10797460  Kepler   CONFIRMED            54.418383   \n",
       "2     K00753.01     10811496  Kepler   CANDIDATE            19.899140   \n",
       "\n",
       "   transit_epoch_bjd  transit_duration_hours  transit_depth_ppm  \\\n",
       "0         170.538750                  2.9575              616.0   \n",
       "1         162.513840                  4.5070              875.0   \n",
       "2         175.850252                  1.7822            10800.0   \n",
       "\n",
       "   planet_radius_re  equilibrium_temp_k  ...  planet_radius_re_imputed  \\\n",
       "0              2.26               793.0  ...                     False   \n",
       "1              2.83               443.0  ...                     False   \n",
       "2             14.60               638.0  ...                     False   \n",
       "\n",
       "   duration_over_period  duration_over_period_clipped  \\\n",
       "0              0.012988                      0.012988   \n",
       "1              0.003451                      0.003451   \n",
       "2              0.003732                      0.003732   \n",
       "\n",
       "   impact_parameter_pred_std  impact_parameter_ci95_low  \\\n",
       "0                        NaN                        NaN   \n",
       "1                        NaN                        NaN   \n",
       "2                        NaN                        NaN   \n",
       "\n",
       "   impact_parameter_ci95_high  impact_parameter_imputed  \\\n",
       "0                         NaN                     False   \n",
       "1                         NaN                     False   \n",
       "2                         NaN                     False   \n",
       "\n",
       "   equilibrium_temp_k_postfill_median  stellar_logg_postfill_median  \\\n",
       "0                               False                         False   \n",
       "1                               False                         False   \n",
       "2                               False                         False   \n",
       "\n",
       "   stellar_teff_k_postfill_median  \n",
       "0                           False  \n",
       "1                           False  \n",
       "2                           False  \n",
       "\n",
       "[3 rows x 41 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load curated dataset\n",
    "data_path = Path('..') / 'data' / 'unified_exoplanets_final_imputed.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "print(df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2af12b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "disposition\n",
       "CANDIDATE         7429\n",
       "FALSE POSITIVE    5827\n",
       "CONFIRMED         4113\n",
       "CP                 679\n",
       "KP                 565\n",
       "FA                  98\n",
       "REFUTED              5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['disposition'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6f01bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definitive label class counts: {0: 5930, 1: 4113}\n",
      "Filtered shape: (10043, 42)\n"
     ]
    }
   ],
   "source": [
    "# Prepare target label with expanded binary mapping and filter definitives\n",
    "\n",
    "def create_binary_labels(labels: pd.Series) -> pd.Series:\n",
    "    binary_map = {\n",
    "        'CONFIRMED': 1,          # Positive class\n",
    "        'FALSE POSITIVE': 0,     # Negative class\n",
    "        'REFUTED': 0,            # Treat as false positive\n",
    "        'FA': 0,                 # False alarm as negative\n",
    "        # Uncertain / not for supervised training\n",
    "        'CANDIDATE': None,\n",
    "        'KP': None,\n",
    "        'CP': None,\n",
    "    }\n",
    "    return labels.map(binary_map)\n",
    "\n",
    "binary_labels = create_binary_labels(df['disposition'])\n",
    "mask = ~binary_labels.isna()\n",
    "\n",
    "# Restrict to definitive labels and attach y\n",
    "df_sup = df.loc[mask].copy()\n",
    "df_sup['label'] = binary_labels.loc[mask].astype(int)\n",
    "print('Definitive label class counts:', df_sup['label'].value_counts().to_dict())\n",
    "print('Filtered shape:', df_sup.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bf22c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped columns: ['stellar_teff_k_ci95_low', 'stellar_radius_rsun_ci95_high', 'impact_parameter_ci95_high', 'impact_parameter_ci95_low', 'ra', 'host_star_id', 'planet_radius_re_ci95_high', 'stellar_radius_rsun_ci95_low', 'dec', 'planet_radius_re_ci95_low', 'mission', 'planet_name', 'disposition', 'stellar_teff_k_ci95_high']\n",
      "Feature count: 27\n"
     ]
    }
   ],
   "source": [
    "# Column selection: drop identifiers and high-leakage columns\n",
    "id_cols = ['planet_name','host_star_id','mission','disposition','ra','dec']\n",
    "ci_cols = [c for c in df_sup.columns if c.endswith('_ci95_low') or c.endswith('_ci95_high')]\n",
    "# Keep uncertainty stds and imputation flags; they can carry useful signal without leaking labels\n",
    "drop_cols = list(set(id_cols + ci_cols))\n",
    "X = df_sup.drop(columns=drop_cols + ['label'], errors='ignore')\n",
    "y = df_sup['label']\n",
    "print('Dropped columns:', drop_cols)\n",
    "print('Feature count:', X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e4acfe",
   "metadata": {},
   "source": [
    "#### Feature engineering block\n",
    "Creates interaction and log features informed by SHAP (radius family), residual depth, and a placeholder FFT feature hook. Includes a PCA toggle to compress the numeric space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1e17c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineered features kept: ['acc_grav_stellar_surface', 'duration_over_period_fe', 'log_transit_depth_ppm']\n",
      "Final feature count: 30\n"
     ]
    }
   ],
   "source": [
    "# Limit engineered features; include acc_grav_stellar_surface (derive from stellar_logg if missing)\n",
    "\n",
    "def add_engineered_features(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_e = df_in.copy()\n",
    "    # Ensure requested physical feature exists\n",
    "    if 'acc_grav_stellar_surface' not in df_e.columns and 'stellar_logg' in df_e.columns:\n",
    "        # Assume stellar_logg in cgs (log10(cm/s^2)); compute g in cm/s^2\n",
    "        df_e['acc_grav_stellar_surface'] = np.power(10.0, pd.to_numeric(df_e['stellar_logg'], errors='coerce'))\n",
    "    elif 'acc_grav_stellar_surface' in df_e.columns:\n",
    "        df_e['acc_grav_stellar_surface'] = pd.to_numeric(df_e['acc_grav_stellar_surface'], errors='coerce').clip(lower=0)\n",
    "\n",
    "    # Keep a very small, stable set of engineered features\n",
    "    # 1) duration/period ratio (bounded)\n",
    "    if 'transit_duration_hours' in df_e.columns and 'orbital_period_days' in df_e.columns:\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            frac = df_e['transit_duration_hours'] / (24.0 * df_e['orbital_period_days'])\n",
    "            df_e['duration_over_period_fe'] = np.clip(frac.replace([np.inf, -np.inf], np.nan), 0.0, 1.0)\n",
    "    # 2) log transform for depth (reduce skew)\n",
    "    if 'transit_depth_ppm' in df_e.columns:\n",
    "        df_e['log_transit_depth_ppm'] = np.log1p(np.clip(df_e['transit_depth_ppm'], a_min=0, a_max=None))\n",
    "    return df_e\n",
    "\n",
    "# Build features then restrict to definitive labels\n",
    "X_all = add_engineered_features(df)\n",
    "X = X_all.loc[mask].copy()\n",
    "y = df_sup['label']\n",
    "\n",
    "# Drop identifiers and known leakage columns; keep uncertainty stds and imputation flags\n",
    "id_cols = ['planet_name','host_star_id','mission','disposition','ra','dec']\n",
    "ci_cols = [c for c in X.columns if c.endswith('_ci95_low') or c.endswith('_ci95_high')]\n",
    "drop_cols = list(set(id_cols + ci_cols + ['label']))\n",
    "X = X.drop(columns=[c for c in drop_cols if c in X.columns], errors='ignore')\n",
    "\n",
    "print('Engineered features kept:', [c for c in ['acc_grav_stellar_surface','duration_over_period_fe','log_transit_depth_ppm'] if c in X.columns])\n",
    "print('Final feature count:', X.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ee79457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits (definitive only): (7030, 30) (1506, 30) (1507, 30)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 0.8467839074921706, 1: 1.220910038207711}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate numeric/categorical and build preprocessing over full definitive dataset\n",
    "numeric_cols = [c for c in X.columns if X[c].dtype.kind in 'bifc']\n",
    "categorical_cols = [c for c in X.columns if X[c].dtype.kind not in 'bifc']\n",
    "preprocess = ColumnTransformer([\n",
    "    ('num', StandardScaler(), numeric_cols),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "], remainder='drop')\n",
    "\n",
    "# Train/val/test across the entire definitive dataset\n",
    "aaa = len(y)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "print('Splits (definitive only):', X_train.shape, X_val.shape, X_test.shape)\n",
    "\n",
    "# Class weights (to address imbalance)\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "classes = np.unique(y_train)\n",
    "class_weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
    "class_weight_dict = {int(k): float(v) for k, v in zip(classes, class_weights)}\n",
    "class_weight_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e4d837",
   "metadata": {},
   "source": [
    "### Optional: MC robustness evaluation\n",
    "We can sample uncertainty-aware columns (e.g., impact_parameter) and retrain/evaluate across several draws, reporting mean Â± std of metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3b2c6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 40 MC frames for robust evaluation\n"
     ]
    }
   ],
   "source": [
    "# Monte Carlo frames loader (expects mc_frames saved or generated upstream).\n",
    "# If not available, this cell will create simple bootstrapped surrogates as a fallback.\n",
    "\n",
    "N_MC_EXPECTED = 40\n",
    "mc_frames = []\n",
    "try:\n",
    "    # If upstream saved MC frames to disk as separate CSVs, you could load them here.\n",
    "    # Placeholder: not implemented; keep mc_frames empty and rely on upstream import below if exists.\n",
    "    pass\n",
    "except Exception as e:\n",
    "    print(\"MC frame load failed:\", e)\n",
    "\n",
    "# Optional: derive MC frames from upstream utilities in the data notebook result if available in memory.\n",
    "# Here, we fallback to simple noise injection on columns with *_pred_std present to mimic MC draws.\n",
    "if not mc_frames:\n",
    "    df_sup = df.copy()\n",
    "    std_cols = [c for c in df_sup.columns if c.endswith(\"_pred_std\")]\n",
    "    base_cols = [c.replace(\"_pred_std\", \"\") for c in std_cols]\n",
    "    rng = np.random.default_rng(42)\n",
    "    for i in range(N_MC_EXPECTED):\n",
    "        dfi = df_sup.copy()\n",
    "        for base, stdc in zip(base_cols, std_cols):\n",
    "            if base in dfi.columns:\n",
    "                noise = rng.normal(0, dfi[stdc].fillna(0).to_numpy())\n",
    "                dfi[base] = (dfi[base].astype(float) + noise).astype(float)\n",
    "        mc_frames.append(dfi)\n",
    "print(f\"Prepared {len(mc_frames)} MC frames for robust evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dbe207",
   "metadata": {},
   "source": [
    "## Save artifacts and metadata\n",
    "\n",
    "We persist the calibrated model, threshold, preprocessing, and MC-robust metrics for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "725c5986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to b:\\Akshat\\GIT and projects\\NASA_SapceApps\\Model\\artifacts\\stacking_calibrated.joblib\n",
      "Saved preprocessing to b:\\Akshat\\GIT and projects\\NASA_SapceApps\\Model\\artifacts\\preprocess.joblib\n",
      "Saved metadata to b:\\Akshat\\GIT and projects\\NASA_SapceApps\\Model\\artifacts\\metadata.json\n"
     ]
    }
   ],
   "source": [
    "from joblib import dump\n",
    "import json\n",
    "\n",
    "artifacts_dir.mkdir(parents=True, exist_ok=True)\n",
    "model_path = artifacts_dir / \"stacking_calibrated.joblib\"\n",
    "preprocess_path = artifacts_dir / \"preprocess.joblib\"\n",
    "meta_path = artifacts_dir / \"metadata.json\"\n",
    "\n",
    "# Pick the trained model object available in the notebook\n",
    "model_to_save = globals().get(\"calib\") or globals().get(\"stack_pipe\") or globals().get(\"best_model\")\n",
    "if model_to_save is None:\n",
    "    raise NameError(\"No trained model found to save. Expected one of 'calib', 'stack_pipe', or 'best_model'.\")\n",
    "\n",
    "# Save model and preprocess\n",
    "try:\n",
    "    dump(model_to_save, model_path)\n",
    "except Exception as e:\n",
    "    print(\"Model save failed:\", e)\n",
    "dump(preprocess, preprocess_path)\n",
    "\n",
    "# Prepare metadata; mc_stack_results may not be computed if MC step was skipped\n",
    "metadata = {\n",
    "    \"model\": \"StackingClassifier + ROS (calibrated isotonic)\" if \"calib\" in globals() else type(model_to_save).__name__,\n",
    "    \"threshold\": globals().get(\"BEST_THRESHOLD\", 0.5),\n",
    "    \"test_metrics\": {\"accuracy\": acc, \"roc_auc\": auc, \"pr_auc\": ap},\n",
    "    \"features\": X.columns.tolist(),\n",
    "}\n",
    "if \"mc_stack_results\" in globals():\n",
    "    metadata[\"mc_results\"] = mc_stack_results\n",
    "\n",
    "with open(meta_path, \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"Saved model to {model_path}\")\n",
    "print(f\"Saved preprocessing to {preprocess_path}\")\n",
    "print(f\"Saved metadata to {meta_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a604b22",
   "metadata": {},
   "source": [
    "## Calibrated Stacking + ROS (final)\n",
    "\n",
    "We keep only the calibrated StackingClassifier with RandomOverSampler. We'll fit on train, calibrate on validation, pick a threshold, and report test performance. Then we run MC-robust evaluation (30â€“50 draws) with 95% CIs and simple error analysis. All other modeling blocks were removed to keep this notebook focused and reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41f2f136",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akshat\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected threshold on validation (robust): 0.500\n",
      "{'accuracy': 0.9296615792966157, 'roc_auc': 0.9746808588130315, 'pr_auc': 0.9558057906650027}\n",
      "Confusion matrix:\n",
      " [[821  69]\n",
      " [ 37 580]]\n"
     ]
    }
   ],
   "source": [
    "# Build Stacking + ROS pipeline\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "# Helper: picklable converter to string for categorical columns\n",
    "\n",
    "def to_str_df(X):\n",
    "    try:\n",
    "        return X.astype(str)\n",
    "    except Exception:\n",
    "        return np.asarray(X).astype(str)\n",
    "\n",
    "# Overwrite preprocess to add imputers (fix NaN handling for LinearSVC and others)\n",
    "num_pipe = Pipeline(steps=[\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"sc\", StandardScaler()),\n",
    "])\n",
    "cat_pipe = Pipeline(steps=[\n",
    "    (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"to_str\", FunctionTransformer(to_str_df)),\n",
    "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "])\n",
    "preprocess = ColumnTransformer([\n",
    "    (\"num\", num_pipe, numeric_cols),\n",
    "    (\"cat\", cat_pipe, categorical_cols),\n",
    "], remainder=\"drop\")\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=400, max_depth=None, random_state=42, n_jobs=-1)\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "svc = LinearSVC(C=0.5, dual=False, max_iter=5000, random_state=42)\n",
    "base_estimators = [\n",
    "    (\"rf\", rf),\n",
    "    (\"gb\", gb),\n",
    "    (\"svc\", svc),\n",
    "]\n",
    "meta = LogisticRegression(max_iter=2000, class_weight=\"balanced\", n_jobs=None)\n",
    "stack = StackingClassifier(estimators=base_estimators, final_estimator=meta, passthrough=False, n_jobs=-1)\n",
    "\n",
    "stack_pipe = ImbPipeline(steps=[\n",
    "    (\"prep\", preprocess),\n",
    "    (\"ros\", RandomOverSampler(random_state=42)),\n",
    "    (\"clf\", stack),\n",
    "])\n",
    "\n",
    "# Fit on train and evaluate on validation\n",
    "stack_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Calibrate on validation\n",
    "calib = CalibratedClassifierCV(estimator=stack_pipe, method=\"isotonic\", cv=\"prefit\")\n",
    "calib.fit(X_val, y_val)\n",
    "\n",
    "# Robust threshold selection on validation\n",
    "# Prefer sweeping unique predicted probabilities with a mild precision constraint to avoid all-positive thresholds\n",
    "\n",
    "def select_threshold(y_true, y_prob, min_precision=0.5, min_recall=None):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_prob = np.asarray(y_prob)\n",
    "    uniq = np.unique(np.concatenate([[0.0, 1.0], y_prob]))\n",
    "    best_thr = 0.5\n",
    "    best_f1 = -1.0\n",
    "    best_pair = (0.0, 0.0)\n",
    "    for t in uniq:\n",
    "        y_hat = (y_prob >= t).astype(int)\n",
    "        tp = np.sum((y_hat == 1) & (y_true == 1))\n",
    "        fp = np.sum((y_hat == 1) & (y_true == 0))\n",
    "        fn = np.sum((y_hat == 0) & (y_true == 1))\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        if min_precision is not None and precision < min_precision:\n",
    "            continue\n",
    "        if min_recall is not None and recall < min_recall:\n",
    "            continue\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        if f1 > best_f1 or (np.isclose(f1, best_f1) and recall > best_pair[1]):\n",
    "            best_f1 = f1\n",
    "            best_thr = t\n",
    "            best_pair = (precision, recall)\n",
    "    if best_f1 >= 0:\n",
    "        return float(best_thr)\n",
    "    # Fallback to PR-curve argmax F1 mapping\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "    f1s = 2 * precision * recall / (precision + recall + 1e-12)\n",
    "    idx = int(np.nanargmax(f1s))\n",
    "    if idx == 0:\n",
    "        return float(np.min(y_prob))\n",
    "    if (idx - 1) < len(thresholds):\n",
    "        return float(thresholds[idx - 1])\n",
    "    return float(np.median(y_prob))\n",
    "\n",
    "val_probs = calib.predict_proba(X_val)[:, 1]\n",
    "BEST_THRESHOLD = select_threshold(y_val, val_probs, min_precision=0.5)\n",
    "print(f\"Selected threshold on validation (robust): {BEST_THRESHOLD:.3f}\")\n",
    "\n",
    "# Evaluate on test\n",
    "probs_test = calib.predict_proba(X_test)[:, 1]\n",
    "preds_test = (probs_test >= BEST_THRESHOLD).astype(int)\n",
    "acc = accuracy_score(y_test, preds_test)\n",
    "auc = roc_auc_score(y_test, probs_test)\n",
    "ap = average_precision_score(y_test, probs_test)\n",
    "print({\"accuracy\": acc, \"roc_auc\": auc, \"pr_auc\": ap})\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, preds_test)\n",
    "print(\"Confusion matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133ae879",
   "metadata": {},
   "source": [
    "## Monte Carlo robust evaluation with 95% CIs\n",
    "\n",
    "We now propagate the upstream imputation uncertainty using multiple MC samples of the dataset. We report mean and 95% confidence intervals for accuracy, ROC AUC, and PR AUC at the selected threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff8a248",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from scipy.stats import t\n",
    "\n",
    "# Ensure we evaluate using the same selected threshold logic\n",
    "\n",
    "def evaluate_stacking_over_mc(mc_frames, base_model, preprocess, best_threshold, n_draws=40, random_state=42):\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    n = min(n_draws, len(mc_frames))\n",
    "    metrics = []\n",
    "\n",
    "    # Local binary mapping consistent with training\n",
    "    binary_map = {\n",
    "        'CONFIRMED': 1,\n",
    "        'FALSE POSITIVE': 0,\n",
    "        'REFUTED': 0,\n",
    "        'FA': 0,\n",
    "    }\n",
    "\n",
    "    for i in range(n):\n",
    "        df_i = mc_frames[i]\n",
    "        # Recreate supervised view and engineered features so columns match X.columns\n",
    "        df_i_sup = df_i[df_i['disposition'].isin(binary_map.keys())].copy()\n",
    "        df_i_sup['label'] = df_i_sup['disposition'].map(binary_map).astype(int)\n",
    "        df_i_sup = add_engineered_features(df_i_sup)\n",
    "\n",
    "        X_i = df_i_sup[X.columns]\n",
    "        y_i = df_i_sup['label']\n",
    "        # Split deterministically to align with earlier split by index\n",
    "        idx = df_i_sup.index\n",
    "        X_train_i = X_i.loc[idx.intersection(X_train.index)]\n",
    "        y_train_i = y_i.loc[X_train_i.index]\n",
    "        X_val_i = X_i.loc[idx.intersection(X_val.index)]\n",
    "        y_val_i = y_i.loc[X_val_i.index]\n",
    "        X_test_i = X_i.loc[idx.intersection(X_test.index)]\n",
    "        y_test_i = y_i.loc[X_test_i.index]\n",
    "\n",
    "        # Fit base model fresh on each draw to propagate uncertainty into training too\n",
    "        m = clone(base_model)\n",
    "        m.fit(X_train_i, y_train_i)\n",
    "        calib_i = CalibratedClassifierCV(estimator=m, method='isotonic', cv='prefit')\n",
    "        calib_i.fit(X_val_i, y_val_i)\n",
    "        probs = calib_i.predict_proba(X_test_i)[:, 1]\n",
    "        preds = (probs >= best_threshold).astype(int)\n",
    "        metrics.append({\n",
    "            'accuracy': accuracy_score(y_test_i, preds),\n",
    "            'roc_auc': roc_auc_score(y_test_i, probs),\n",
    "            'pr_auc': average_precision_score(y_test_i, probs),\n",
    "        })\n",
    "\n",
    "    # Aggregate with 95% t-intervals\n",
    "    def mean_ci(vals):\n",
    "        arr = np.array(vals, dtype=float)\n",
    "        m = float(np.mean(arr))\n",
    "        s = float(np.std(arr, ddof=1)) if len(arr) > 1 else 0.0\n",
    "        ci = t.ppf(0.975, df=max(len(arr)-1,1)) * (s / sqrt(max(len(arr),1))) if len(arr) > 1 else 0.0\n",
    "        return m, m-ci, m+ci\n",
    "\n",
    "    accs = [d['accuracy'] for d in metrics]\n",
    "    rocs = [d['roc_auc'] for d in metrics]\n",
    "    prs = [d['pr_auc'] for d in metrics]\n",
    "    results = {\n",
    "        'n_draws': n,\n",
    "        'accuracy': {'mean': mean_ci(accs)[0], 'ci95_low': mean_ci(accs)[1], 'ci95_high': mean_ci(accs)[2]},\n",
    "        'roc_auc': {'mean': mean_ci(rocs)[0], 'ci95_low': mean_ci(rocs)[1], 'ci95_high': mean_ci(rocs)[2]},\n",
    "        'pr_auc': {'mean': mean_ci(prs)[0], 'ci95_low': mean_ci(prs)[1], 'ci95_high': mean_ci(prs)[2]},\n",
    "    }\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b7120c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 0.8936825885978429, 'recall': 0.940032414910859, 'f1': 0.9162717219589257}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# Ensure predictions exist\n",
    "if 'probs_test' not in globals():\n",
    "    probs_test = calib.predict_proba(X_test)[:, 1]\n",
    "if 'BEST_THRESHOLD' not in globals():\n",
    "    BEST_THRESHOLD = 0.5\n",
    "preds_test = (probs_test >= BEST_THRESHOLD).astype(int)\n",
    "\n",
    "precision = precision_score(y_test, preds_test)\n",
    "recall = recall_score(y_test, preds_test)\n",
    "f1 = f1_score(y_test, preds_test)\n",
    "\n",
    "print({'precision': precision, 'recall': recall, 'f1': f1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0871022",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
