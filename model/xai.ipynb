{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2911e971",
   "metadata": {},
   "source": [
    "# XAI: quantitative SHAP analysis and Gemini qualitative explanation\n",
    "\n",
    "This notebook: loads the trained stacking model artifacts, prepares an input sample, computes prediction and SHAP-based quantitative analysis, then sends a structured prompt to a text-generation API (Gemini) to produce a technical, qualitative explanation.\n",
    "\n",
    "Requirements: install `pandas`, `numpy`, `scikit-learn`, `shap`, `requests`, and optionally `python-dotenv`. Ensure model artifacts exist in `models/` and set `GEMINI_API_KEY` (or use the `scripts/set_gemini_key.ps1` script)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b95753d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imports ok\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "print('imports ok')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62629d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".env loaded (if present)\n",
      "GEMINI_API_KEY present: yes\n"
     ]
    }
   ],
   "source": [
    "# Load .env if available and try to read user env (Windows) so setx changes are visible\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    print('.env loaded (if present)')\n",
    "except Exception:\n",
    "    print('python-dotenv not installed; skipping .env load')\n",
    "\n",
    "# Try to ensure GEMINI_API_KEY present by reading HKCU\\Environment (Windows) if needed\n",
    "if not os.environ.get('GEMINI_API_KEY'):\n",
    "    try:\n",
    "        import winreg\n",
    "        with winreg.OpenKey(winreg.HKEY_CURRENT_USER, 'Environment') as reg:\n",
    "            try:\n",
    "                val, _ = winreg.QueryValueEx(reg, 'GEMINI_API_KEY')\n",
    "                if val:\n",
    "                    os.environ['GEMINI_API_KEY'] = val\n",
    "                    print('GEMINI_API_KEY loaded from HKCU\\\\Environment')\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "    except Exception as e:\n",
    "        print('Could not read Windows registry for GEMINI_API_KEY:', e)\n",
    "\n",
    "print('GEMINI_API_KEY present:', 'yes' if os.environ.get('GEMINI_API_KEY') else 'no')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dc4884a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "sample_data = {\n",
    "    \"orbital_period_days\": [5.72],\n",
    "    \"transit_epoch_bjd\": [2457000.12345],\n",
    "    \"transit_duration_hours\": [2.1],\n",
    "    \"transit_depth_ppm\": [1300.0],\n",
    "    \"planet_radius_re\": [1.12],\n",
    "    \"equilibrium_temp_k\": [1100.0],\n",
    "    \"insolation_flux\": [800.0],\n",
    "    \"impact_parameter\": [0.45],\n",
    "    \"stellar_teff_k\": [5700.0],\n",
    "    \"stellar_radius_rsun\": [0.98],\n",
    "    \"stellar_radius_normal\": [1.00],\n",
    "    \"stellar_mass_msun\": [1.02],\n",
    "    \"mass_rad_ratio\": [1.04],\n",
    "    \"stellar_logg\": [4.38],\n",
    "    \"acc_grav_stellar_surface\": [2.4e4],\n",
    "    \"ra\": [299.123],\n",
    "    \"dec\": [45.789],\n",
    "    \"radius_ratio_est\": [0.011]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "sample_row = pd.DataFrame(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc09760a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models_dir -> d:\\College Files\\Hackathons\\NasaSpaceApps\\spaceapps_exoplanet_detection\\model\\models\n"
     ]
    }
   ],
   "source": [
    "# Determine models directory\n",
    "base = os.path.abspath(os.getcwd())\n",
    "models_dir = os.path.join(base, 'models')\n",
    "# Check current directory's 'models' first\n",
    "if not os.path.exists(models_dir):\n",
    "    # Check parent directory's 'models' if not found\n",
    "    potential_models_dir = os.path.join(base, '..', 'models')\n",
    "    if os.path.exists(potential_models_dir):\n",
    "        models_dir = potential_models_dir\n",
    "    else:\n",
    "        # If neither path exists, default back to the first one and rely on load to fail\n",
    "        pass \n",
    "        \n",
    "print('models_dir ->', models_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcdcad5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\laxmi prajapati\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\laxmi prajapati\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\laxmi prajapati\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (2.2.6)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\laxmi prajapati\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\laxmi prajapati\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11fdec8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded models with joblib!\n"
     ]
    }
   ],
   "source": [
    "import joblib # <--- Import joblib\n",
    "import os\n",
    "# ...\n",
    "\n",
    "# Load trained model\n",
    "# Note: joblib.load can typically handle the path directly, \n",
    "# so you often don't need to manually open the file with 'with open...'\n",
    "model = joblib.load('../models/stacking_classifier_FIXED.pkl')\n",
    "scaler = joblib.load('../models/scaler_FIXED.pkl')\n",
    "selector = joblib.load('../models/selector_FIXED.pkl')\n",
    "\n",
    "print(\"Successfully loaded models with joblib!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "914d0a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load selected features if available\n",
    "for name in ['selected_features.pkl', 'selected_features']:\n",
    "    path = os.path.join(models_dir, name)\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            selected_features = joblib.load(path)\n",
    "            break\n",
    "        except Exception:\n",
    "            # Silently ignore if file exists but load fails, as original code did\n",
    "            pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7593f8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features used to fit the scaler (Name and Order):\n",
      "  1. orbital_period_days\n",
      "  2. transit_epoch_bjd\n",
      "  3. transit_duration_hours\n",
      "  4. transit_depth_ppm\n",
      "  5. planet_radius_re\n",
      "  6. equilibrium_temp_k\n",
      "  7. insolation_flux\n",
      "  8. impact_parameter\n",
      "  9. stellar_teff_k\n",
      "  10. stellar_radius_rsun\n",
      "  11. stellar_radius_normal\n",
      "  12. stellar_mass_msun\n",
      "  13. mass_rad_ratio\n",
      "  14. stellar_logg\n",
      "  15. acc_grav_stellar_surface\n",
      "  16. ra\n",
      "  17. dec\n",
      "  18. radius_ratio_est\n"
     ]
    }
   ],
   "source": [
    "# Assuming your scaling object is named 'scaler'\n",
    "try:\n",
    "    fitted_features = scaler.feature_names_in_\n",
    "    print(\"Features used to fit the scaler (Name and Order):\")\n",
    "    for i, name in enumerate(fitted_features):\n",
    "        print(f\"  {i+1}. {name}\")\n",
    "except AttributeError:\n",
    "    print(\"The scaler does not have the 'feature_names_in_' attribute (it may have been fitted on a NumPy array).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53a2e3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Intermediate Data Shapes ---\n",
      "X_scaled shape (18 features): (1, 18)\n",
      "X_selected shape (Selected features): (1, 12)\n",
      "\n",
      "--- Prediction Results ---\n",
      "prediction: label=0, prob=0.0315\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib \n",
    "import os\n",
    "\n",
    "# --- 1. The Definitive 18-Feature List ---\n",
    "FINAL_SCALER_FEATURES = [\n",
    "    \"orbital_period_days\", \"transit_epoch_bjd\", \"transit_duration_hours\",\n",
    "    \"transit_depth_ppm\", \"planet_radius_re\", \"equilibrium_temp_k\", \n",
    "    \"insolation_flux\", \"impact_parameter\", \"stellar_teff_k\", \n",
    "    \"stellar_radius_rsun\", \"stellar_radius_normal\", \"stellar_mass_msun\", \n",
    "    \"mass_rad_ratio\", \"stellar_logg\", \"acc_grav_stellar_surface\", \n",
    "    \"ra\", \"dec\", \"radius_ratio_est\"\n",
    "]\n",
    "\n",
    "# --- 2. Load Trained Model Objects (Assuming success) ---\n",
    "try:\n",
    "    model = joblib.load('../models/stacking_classifier_FIXED.pkl')\n",
    "    scaler = joblib.load('../models/scaler_FIXED.pkl')\n",
    "    selector = joblib.load('../models/selector_FIXED.pkl')\n",
    "except Exception as e:\n",
    "    print(\"Error loading models. Please ensure '../models/' path is correct.\")\n",
    "    # Re-raise the error so you see the specific problem\n",
    "    raise\n",
    "\n",
    "\n",
    "# --- 3. The Final, Corrected Prediction Function (MODIFIED) ---\n",
    "\n",
    "def predict_exoplanet_robust(data: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Returns X_scaled, X_selected, and the prediction probability.\n",
    "    \"\"\"\n",
    "    X_processed = data.copy()\n",
    "    \n",
    "    # Feature Engineering\n",
    "    X_processed.loc[:, 'acc_grav_stellar_surface'] = 10**X_processed['stellar_logg']\n",
    "\n",
    "    # Select the FINAL 18 Features in the CORRECT ORDER\n",
    "    X_final = X_processed[FINAL_SCALER_FEATURES].fillna(0) \n",
    "    \n",
    "    # Apply Pipeline Steps\n",
    "    X_scaled = scaler.transform(X_final)\n",
    "    X_selected = selector.transform(X_scaled)\n",
    "    \n",
    "    # Predict\n",
    "    pred_proba = float(model.predict_proba(X_selected)[:, 1][0])\n",
    "    \n",
    "    # MODIFIED: Return the intermediate arrays and the prediction\n",
    "    return X_scaled, X_selected, pred_proba\n",
    "\n",
    "\n",
    "# --- 4. Prepare and Run Test Data ---\n",
    "sample_data = {\n",
    "    \"orbital_period_days\": [5.72], \"transit_epoch_bjd\": [2457000.12345], \n",
    "    \"transit_duration_hours\": [2.1], \"transit_depth_ppm\": [1300.0], \n",
    "    \"planet_radius_re\": [1.12], \"equilibrium_temp_k\": [1100.0], \n",
    "    \"insolation_flux\": [800.0], \"impact_parameter\": [0.45], \n",
    "    \"stellar_teff_k\": [5700.0], \"stellar_radius_rsun\": [0.98], \n",
    "    \"stellar_radius_normal\": [1.0], \"stellar_mass_msun\": [1.02], \n",
    "    \"mass_rad_ratio\": [1.04], \"stellar_logg\": [4.38], \n",
    "    \"ra\": [299.123], \"dec\": [45.789], \"radius_ratio_est\": [0.011]\n",
    "}\n",
    "\n",
    "test_sample = pd.DataFrame(sample_data)\n",
    "\n",
    "# Call the modified prediction function\n",
    "try:\n",
    "    X_scaled, X_selected, pred_proba = predict_exoplanet_robust(test_sample)\n",
    "    pred_label = int(pred_proba >= 0.5)\n",
    "\n",
    "    print(f'\\n--- Intermediate Data Shapes ---')\n",
    "    print(f'X_scaled shape (18 features): {X_scaled.shape}')\n",
    "    print(f'X_selected shape (Selected features): {X_selected.shape}')\n",
    "\n",
    "    print(f'\\n--- Prediction Results ---')\n",
    "    print(f'prediction: label={pred_label}, prob={pred_proba:.4f}')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- EXECUTION FAILED ---\")\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1522078d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. ADD THESE TWO LINES HERE:\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ded69ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Laxmi Prajapati\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TreeExplainer failed, falling back to KernelExplainer: Model type not yet supported by TreeExplainer: <class 'sklearn.ensemble._stacking.StackingClassifier'>\n",
      "Preparing robust background dataset from training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: SHAP array length (24) does not match feature count (12). Slicing to the correct length.\n",
      "SHAP computed; top contributors:\n",
      "transit_epoch_bjd           0.100368\n",
      "stellar_logg                0.021352\n",
      "radius_ratio_est            0.006399\n",
      "acc_grav_stellar_surface    0.002989\n",
      "stellar_teff_k              0.000000\n",
      "equilibrium_temp_k          0.000000\n",
      "transit_depth_ppm           0.000000\n",
      "impact_parameter            0.000000\n",
      "ra                         -0.002989\n",
      "dec                        -0.006399\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the feature list needed for mapping\n",
    "FINAL_SCALER_FEATURES = [\n",
    "    \"orbital_period_days\", \"transit_epoch_bjd\", \"transit_duration_hours\",\n",
    "    \"transit_depth_ppm\", \"planet_radius_re\", \"equilibrium_temp_k\", \n",
    "    \"insolation_flux\", \"impact_parameter\", \"stellar_teff_k\", \n",
    "    \"stellar_radius_rsun\", \"stellar_radius_normal\", \"stellar_mass_msun\", \n",
    "    \"mass_rad_ratio\", \"stellar_logg\", \"acc_grav_stellar_surface\", \n",
    "    \"ra\", \"dec\", \"radius_ratio_est\"\n",
    "]\n",
    "selected_features = FINAL_SCALER_FEATURES \n",
    "\n",
    "# Compute SHAP explanations (FIXED)\n",
    "import shap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    # --- Original TreeExplainer attempt ---\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_vals = explainer.shap_values(X_selected)\n",
    "    if isinstance(shap_vals, list):\n",
    "        shap_vals = shap_vals[1]\n",
    "    \n",
    "    # FIX APPLIED in previous step: Flatten the array\n",
    "    shap_for_sample = np.array(shap_vals[0]).flatten() \n",
    "    \n",
    "except Exception as e:\n",
    "    # --- Robust KernelExplainer Fallback ---\n",
    "    print('TreeExplainer failed, falling back to KernelExplainer:', e)\n",
    "    \n",
    "    # 1. Prepare Background Dataset from Training Data\n",
    "    print('Preparing robust background dataset from training data...')\n",
    "    \n",
    "    # CRITICAL: Ensure 'unified_exoplanets_final_imputed.csv' is in the correct directory.\n",
    "    # Note: If running locally, you must ensure the path is correct for your system.\n",
    "    # The path was changed to a simple filename in the previous step, use the one that works for you.\n",
    "    try:\n",
    "        df_train = pd.read_csv('unified_exoplanets_final_imputed.csv')\n",
    "    except FileNotFoundError:\n",
    "        # Fallback to the relative path if the simple filename doesn't work locally\n",
    "        df_train = pd.read_csv('../data/unified_exoplanets_final_imputed.csv')\n",
    "\n",
    "\n",
    "    df_train.loc[:, 'acc_grav_stellar_surface'] = 10**df_train['stellar_logg']\n",
    "    X_train_final = df_train[FINAL_SCALER_FEATURES].fillna(0)\n",
    "    \n",
    "    # 2. Apply Scaling and Selection to Training Data\n",
    "    X_train_scaled = scaler.transform(X_train_final)\n",
    "    X_train_selected = selector.transform(X_train_scaled)\n",
    "    \n",
    "    background = shap.sample(X_train_selected, min(100, X_train_selected.shape[0]))\n",
    "    \n",
    "    # 3. Run KernelExplainer\n",
    "    explainer = shap.KernelExplainer(model.predict_proba, background)\n",
    "    shap_vals = explainer.shap_values(X_selected, nsamples=100)\n",
    "    \n",
    "    if isinstance(shap_vals, list):\n",
    "        shap_vals = shap_vals[1]\n",
    "    \n",
    "    # FIX APPLIED in previous step: Flatten the array\n",
    "    shap_for_sample = np.array(shap_vals[0]).flatten() \n",
    "\n",
    "# Map back to feature names\n",
    "try:\n",
    "    support = selector.get_support(indices=True)\n",
    "    feature_names_selected = [selected_features[i] for i in support]\n",
    "except Exception:\n",
    "    # If selector fails, we rely on the length of the SHAP output, which is the source of the error.\n",
    "    # We must assume the selector worked and report a warning if this part is reached.\n",
    "    print(\"WARNING: Feature selection failed. Using an estimated length for slicing.\")\n",
    "    feature_names_selected = selected_features[:len(shap_for_sample) // 2] \n",
    "\n",
    "\n",
    "# --- FINAL FIX FOR LENGTH MISMATCH ---\n",
    "n_expected_features = len(feature_names_selected)\n",
    "if len(shap_for_sample) != n_expected_features:\n",
    "    print(f\"WARNING: SHAP array length ({len(shap_for_sample)}) does not match feature count ({n_expected_features}). Slicing to the correct length.\")\n",
    "    # Slice the SHAP array to the correct length (assuming the correct values are in the first half)\n",
    "    shap_for_sample = shap_for_sample[:n_expected_features]\n",
    "\n",
    "\n",
    "shap_series = pd.Series(shap_for_sample, index=feature_names_selected).sort_values(ascending=False)\n",
    "print('SHAP computed; top contributors:')\n",
    "print(shap_series.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffccfea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-genai in c:\\users\\laxmi prajapati\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.41.0)\n",
      "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in c:\\users\\laxmi prajapati\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-genai) (9.1.2)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in c:\\users\\laxmi prajapati\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-genai) (4.11.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.1 in c:\\users\\laxmi prajapati\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-genai) (2.32.5)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in c:\\users\\laxmi prajapati\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-genai) (2.11.10)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in c:\\users\\laxmi prajapati\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-genai) (2.41.1)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in c:\\users\\laxmi prajapati\\appdata\\roaming\\python\\python310\\site-packages (from google-genai) (4.15.0)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in c:\\users\\laxmi prajapati\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-genai) (15.0.1)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in c:\\users\\laxmi prajapati\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-genai) (0.28.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\laxmi prajapati\\appdata\\roaming\\python\\python310\\site-packages (from anyio<5.0.0,>=4.8.0->google-genai) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\laxmi prajapati\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio<5.0.0,>=4.8.0->google-genai) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\laxmi prajapati\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.10)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\laxmi prajapati\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (4.9.1)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in c:\\users\\laxmi prajapati\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (6.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\laxmi prajapati\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (0.4.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\laxmi prajapati\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
      "Requirement already satisfied: certifi in c:\\users\\laxmi prajapati\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2025.10.5)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\laxmi prajapati\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\laxmi prajapati\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.4.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\laxmi prajapati\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\laxmi prajapati\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (2.33.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\laxmi prajapati\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.5.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\laxmi prajapati\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.4.3)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\users\\laxmi prajapati\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai) (0.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4379449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top quantitative contributors:\n",
      "                 feature        value  shap_value  abs_shap  pct_contrib\n",
      "  transit_duration_hours 2.100000e+00   -0.100368  0.100368    38.276851\n",
      "       transit_epoch_bjd 2.457000e+06    0.100368  0.100368    38.276851\n",
      "            stellar_logg 4.380000e+00    0.021352  0.021352     8.142995\n",
      "       stellar_mass_msun 1.020000e+00   -0.021352  0.021352     8.142995\n",
      "                     dec 4.578900e+01   -0.006399  0.006399     2.440400\n",
      "        radius_ratio_est 1.100000e-02    0.006399  0.006399     2.440400\n",
      "                      ra 2.991230e+02   -0.002989  0.002989     1.139754\n",
      "acc_grav_stellar_surface          NaN    0.002989  0.002989     1.139754\n",
      "prompt preview:\n",
      " Analyze the following machine learning prediction data and SHAP values and return the explanation for the prediction:\n",
      "\n",
      "{\n",
      "  \"dataset\": \"unified_exoplanets_final_imputed.csv\",\n",
      "  \"model\": \"stacking ensemble (RF+GB+SVM+LR) with feature selection + scaling\",\n",
      "  \"prediction\": {\n",
      "    \"label\": 0,\n",
      "    \"probability\": 0.03146702288316962\n",
      "  },\n",
      "  \"quantitative_shap_top\": [\n",
      "    {\n",
      "      \"feature\": \"transit_duration_hours\",\n",
      "      \"value\": 2.1,\n",
      "      \"shap_value\": -0.10036776562837833,\n",
      "      \"abs_shap\": 0.10036776562837833,\n",
      "      \"pct_contrib\": 38.27685091605825\n",
      "    },\n",
      "    {\n",
      "      \"feature\": \"transit_epoch_bjd\",\n",
      "      \"value\": 2457000.12345,\n",
      "      \"shap_value\": 0.10036776562837811,\n",
      "      \"abs_shap\": 0.10036776562837811,\n",
      "      \"pct_contrib\": 38.276850916058166\n",
      "    },\n",
      "    {\n",
      "      \"feature\": \"stellar_logg\",\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "# Quantitative summary and Gemini qualitative explanation\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# NOTE: Ensure shap_series, pred_label, pred_proba, and test_sample (or sample_row) are available from previous steps.\n",
    "\n",
    "TOP_K = 8\n",
    "df_shap = pd.DataFrame({'feature': shap_series.index, 'shap_value': shap_series.values})\n",
    "df_shap['abs_shap'] = df_shap['shap_value'].abs()\n",
    "df_shap = df_shap.sort_values('abs_shap', ascending=False).reset_index(drop=True)\n",
    "total_abs = df_shap['abs_shap'].sum() if df_shap['abs_shap'].sum() != 0 else 1.0\n",
    "df_shap['pct_contrib'] = df_shap['abs_shap'] / total_abs * 100.0\n",
    "\n",
    "def safe_value(f):\n",
    "    try:\n",
    "        # Assuming your prediction sample is named 'test_sample' from the earlier code\n",
    "        return float(test_sample[f].values[0])\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "df_shap['value'] = df_shap['feature'].apply(safe_value)\n",
    "\n",
    "# Top contributors\n",
    "print('Top quantitative contributors:')\n",
    "print(df_shap.head(TOP_K)[['feature','value','shap_value','abs_shap','pct_contrib']].to_string(index=False))\n",
    "\n",
    "summary_stats = {\n",
    "    'prediction': {'label': int(pred_label), 'probability': float(pred_proba)},\n",
    "    'num_reported_features': int(min(TOP_K, len(df_shap))),\n",
    "    'total_abs_shap': float(total_abs),\n",
    "    'positive_shap_sum': float(df_shap[df_shap['shap_value']>0]['shap_value'].sum()),\n",
    "    'negative_shap_sum': float(df_shap[df_shap['shap_value']<0]['shap_value'].sum())\n",
    "}\n",
    "\n",
    "structured = {\n",
    "    'dataset': 'unified_exoplanets_final_imputed.csv',\n",
    "    'model': 'stacking ensemble (RF+GB+SVM+LR) with feature selection + scaling',\n",
    "    'prediction': summary_stats['prediction'],\n",
    "    'quantitative_shap_top': df_shap.head(TOP_K)[['feature','value','shap_value','abs_shap','pct_contrib']].to_dict(orient='records'),\n",
    "    'summary_stats': summary_stats\n",
    "}\n",
    "\n",
    "human_instructions = (\n",
    "    'You are a senior ML engineer. Given the quantitative SHAP analysis (JSON) and the sample feature values, '\n",
    "    'produce a concise technical explanation (5-8 bullet points) describing why the model produced the prediction, '\n",
    "    'include likely causes, model confidence caveats, and 3 concrete suggestions to validate or improve model reliability. '\n",
    "    'Reference the top contributing features and their directional effects. Keep the explanation technical and targeted to a data-science audience.'\n",
    ")\n",
    "\n",
    "# --- Replacement for the original 'prompt =' line ---\n",
    "data_preamble = \"Analyze the following machine learning prediction data and SHAP values and return the explanation for the prediction:\"\n",
    "data_json = json.dumps(structured, indent=2)\n",
    "\n",
    "prompt = (\n",
    "    f\"{data_preamble}\\n\\n{data_json}\\n\\n\"\n",
    "    f\"{human_instructions}\"\n",
    ")\n",
    "# --- End of Replacement ---\n",
    "\n",
    "print('prompt preview:\\n', prompt[:800]) # Keep your preview line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3de0e72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Sending data to gemini-2.0-flash for analysis (using SDK)...\n",
      "\n",
      "--- Gemini Explanation Successfully Generated (via SDK) ---\n",
      "Here's a technical explanation of the model's prediction based on the provided SHAP analysis:\n",
      "\n",
      "*   **Prediction and Confidence:** The stacking ensemble model predicts a label of 0 (likely indicating \"not an exoplanet\") with a low probability of 0.031. This suggests low confidence in the negative classification.\n",
      "\n",
      "*   **Dominant Feature Influence:** The top two features, `transit_duration_hours` and `transit_epoch_bjd`, contribute most significantly (38.3% each) to the prediction. `transit_duration_hours` with a value of 2.1 hours pushes the prediction towards label 0 (negative SHAP value), while `transit_epoch_bjd` with a value of 2457000.12345 pushes the prediction towards label 1 (positive SHAP value). The model is highly sensitive to these features.\n",
      "\n",
      "*   **Secondary Feature Effects:** `stellar_logg` (4.38) increases the probability of label 1, while `stellar_mass_msun` (1.02) decreases it. These features have a smaller, but still noticeable, impact (8.1% each).\n",
      "\n",
      "*   **Minor Feature Effects:** `dec`, `radius_ratio_est`, `ra`, and `acc_grav_stellar_surface` have relatively small contributions to the prediction (2.4%, 2.4%, 1.1%, and 1.1% respectively).\n",
      "\n",
      "*   **Missing Value Handling:** The `acc_grav_stellar_surface` feature has a missing value (NaN). While the imputation strategy is not specified, the model still uses this feature, potentially introducing bias if the imputation is not accurate.\n",
      "\n",
      "*   **Feature Interactions:** The SHAP values only represent the marginal contribution of each feature. There may be significant interaction effects between features that are not captured in this analysis.\n",
      "\n",
      "*   **Model Confidence Caveats:** The low prediction probability suggests the model is uncertain. The opposing effects of `transit_duration_hours` and `transit_epoch_bjd` may indicate conflicting signals in the data, contributing to the low confidence.\n",
      "\n",
      "**Recommendations for Validation and Improvement:**\n",
      "\n",
      "1.  **Investigate Feature Interactions:** Perform a SHAP interaction analysis to identify and quantify the interaction effects between the top features (e.g., `transit_duration_hours` and `transit_epoch_bjd`). This could reveal non-linear relationships the model is exploiting.\n",
      "2.  **Address Missing Values:** Explore different imputation strategies for `acc_grav_stellar_surface` (e.g., using a more sophisticated imputation method or removing the feature if it's consistently missing). Evaluate the impact of different imputation methods on model performance and stability.\n",
      "3.  **Calibrate Prediction Probabilities:** Apply probability calibration techniques (e.g., Platt scaling or isotonic regression) to improve the reliability of the predicted probabilities. This will help ensure that the predicted probabilities accurately reflect the model's confidence.\n",
      "\n",
      "\n",
      "Saved final explanation to output/xai_explanation.txt\n",
      "Saved output/xai_prompt.json for manual submission\n"
     ]
    }
   ],
   "source": [
    "# --- ADD THESE IMPORTS AT THE VERY TOP OF YOUR SCRIPT ---\n",
    "from google import genai\n",
    "from google.genai.errors import APIError\n",
    "# --------------------------------------------------------\n",
    "\n",
    "# ... (rest of your code, calculating structured and prompt) ...\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# CORRECTED API CALL BLOCK\n",
    "# --------------------------------------------------------\n",
    "\n",
    "# Check if API Key is available\n",
    "api_key = os.environ.get('GEMINI_API_KEY')\n",
    "\n",
    "if not api_key:\n",
    "    print('GEMINI_API_KEY not configured; saving prompt for manual submission')\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    with open('output/xai_prompt.json','w',encoding='utf-8') as pf:\n",
    "        json.dump({'prompt': prompt, 'structured': structured}, pf, indent=2)\n",
    "    print('Saved output/xai_prompt.json')\n",
    "\n",
    "else:\n",
    "    try:\n",
    "        # 1. Initialize the client (it automatically detects and uses the API_KEY)\n",
    "        client = genai.Client() \n",
    "        model_name = 'gemini-2.0-flash' # Define model name here\n",
    "        \n",
    "        print(f\"[*] Sending data to {model_name} for analysis (using SDK)...\")\n",
    "\n",
    "        # 2. Use client.models.generate_content and pass the model_name directly\n",
    "        response = client.models.generate_content(\n",
    "            model=model_name, # <-- CORRECTED SYNTAX: Pass model name here\n",
    "            contents=prompt,\n",
    "            config={'max_output_tokens': 768, 'temperature': 0.15}\n",
    "        )\n",
    "        \n",
    "        # Access the text directly\n",
    "        explanation_text = getattr(response, \"text\", None)\n",
    "\n",
    "        if not explanation_text:\n",
    "            print(\"Error: Gemini did not return any text in the response. Check safety settings.\")\n",
    "            explanation_text = \"ERROR: No text received from API.\"\n",
    "        \n",
    "        print('\\n--- Gemini Explanation Successfully Generated (via SDK) ---')\n",
    "        print(explanation_text)\n",
    "        \n",
    "        os.makedirs('output', exist_ok=True) \n",
    "        with open('output/xai_explanation.txt','w',encoding='utf-8') as of:\n",
    "            of.write(explanation_text)\n",
    "        print('\\nSaved final explanation to output/xai_explanation.txt')\n",
    "\n",
    "        os.makedirs('output', exist_ok=True) \n",
    "        with open('output/xai_prompt.json','w',encoding='utf-8') as pf:\n",
    "            json.dump({'prompt': prompt, 'structured': structured}, pf, indent=2)\n",
    "        print('Saved output/xai_prompt.json for manual submission')\n",
    "\n",
    "    except APIError as e:\n",
    "        print(f'\\nAPI call failed (SDK Error): {e}')\n",
    "        print('The API key is likely invalid or restricted. You must fix the 401 error.')\n",
    "        \n",
    "        # Fallback to saving prompt for manual submission\n",
    "        os.makedirs('output', exist_ok=True) \n",
    "        with open('output/xai_prompt.json','w',encoding='utf-8') as pf:\n",
    "            json.dump({'prompt': prompt, 'structured': structured}, pf, indent=2)\n",
    "        print('Saved output/xai_prompt.json for manual submission')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'\\nAn unexpected error occurred: {e}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
